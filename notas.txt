o PROBLEMA ATUAL DE 0 DATANODES RODANDO É CAUSADO PELA PERSISTÊNCIA DOS DADOS, DESCOBRIR COMO RESSOLVER
-> Era causado pelo fato da formatação do namenode e a permanência das informações do namenode deletado nos datanodes
--> foi resolvido pela checagem da existência de dados na pasta e caso verdadeiro, a não formatação do namenode

O BOOTSTRAP DO SPARK NÃO RODOU TAMBÉM
- o que provávelmente causou o problema de não conseguir iniciar o spark context, pq os jars necessários não forram achados, uma vez que nunca foram movidos para o HDFS
-> O problema era justamente esse
--> Resolvido agardando 1 min após o HDFS iniciar para o mesmo sair do safemode e aí sim passar os jars para o lugar configurado no HDFS.

PROBLEMA ATUAL É CAUSADO PELO JOB NÃO CONSEGUIR SER EXECUTADO PELO yarn
-Fala pra checar se há recursos suficientes, ams com certeza há, provavelmente é problema de localização
> tentar adicionar o 0.0.0.0
-> não funcionou
> Provavelmente há erro na configuração dos recursos checar todos
---> o problema está no spark.executor.memory, quando coloco 2g não funciona
******RESOLVER PROBLEMA DO LIMITE DE 1G spark.executor.memory
-->> Na real o problema está na alocação de todos os recursos, provavelmente na divisão entre Yarn e Spark, map reduce vs sparkjob


SOBRE A QUESTÃO DO KERNEL, EU PROVAVELMENTE QUERO CRIAR UM CUSTOMIZADO QUE RODE SEMPRE A CRIAÇÃO DO SPARK CONTEXT
resolvi escrevendo um kernel customizado em json para o jupyter, adicionando o pyspark-shell.py como variável de ambiente "PYTHONSTARTUP"

Agora falta automatizar esse processo
--> Automatizado no dockerfile

A necessidade de rebuildar é causada pelo This is called layer caching.

spark cria um executor um para cada node, tentar limitar a 1 por cluster (Parece que não é possível)

manter arquitetura e somente adicionar mais data nodes quando quiser mais memória (não funciona para o pandas_df mas sim para o spark_df)
-> Para ler os dados do enem pelo spark é necessário pelo menos dois executores de 896mb
--> Isso pode ser feito Aumentando os recursos de um datanode ou adicionando outro datanode

com dois datanodes yarn.nodemanager.resource.memory-mb 896 é suficiente

Só consigo utilizar o spark com dois executores (ler arquivos), não sei o motivo, de qualquer forma o melhor parece ser manter os datanodes com 1gb de memória total considerando os
 896 de cada executor. Isso dá uma margem e ajuda na escalabilidade com mais nodes multiplos de 1gb. Importante lembrar é que apesar desses valores das configurações
 dificilmente os mesmos são atingidos, com a leitura de arquivos de 1gb com spark levando a memória dos datanodes até 500mb

 Não eram dois executores!!!!!!! era o aplication master spark e o executor spark, os dois precisam rodar no cluster para o Spark funcionar (spark no modo client) e os dois tem 
 o memoryOverhead de 384, logo estava realmente faltando recursos
