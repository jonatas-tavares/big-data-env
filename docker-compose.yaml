version: "3.7"

services:
  
  namenode:
    image: jontavpess/hadoop-base:latest
    container_name: namenode
    hostname: namenode
    volumes:
      - ./input:/usr/input
      - ./user_data/hdfs/name:/usr/hadoop/dfs/name
    ports:
      - "8020:8020"
      - "8088:8088"
      - "9870:9870" 
    deploy:
      resources:
        limits:
          memory: 512m

  datanode-1:
    image: jontavpess/hadoop-base:latest
    container_name: datanode-1
    hostname: datanode-1
    volumes:
      - ./user_data/hdfs/data:/usr/hadoop/dfs/data
    ports:
      - "8042:8042"
    depends_on:
      - namenode
    deploy:
      resources:
        limits:
          memory: 1152m

  # datanode-2:
  #   image: jontavpess/hadoop-base:latest
  #   container_name: datanode-2
  #   hostname: datanode-2
  #   volumes:
  #     - ./user_data/hdfs/data:/usr/hadoop/dfs/data
  #   ports:
  #     - "8043:8042"
  #   depends_on:
  #     - "namenode"
  #   deploy:
  #     resources:
  #       limits:
  #         memory: 1152m

  jupyter-spark:
    image: jontavpess/jupyter-spark-base:latest
    container_name: jupyter-spark
    hostname: jupyter-spark
    volumes:
      - ./input:/usr/input
      - ./user_data/jupyter:/mnt/jupyter
    depends_on:
     - "namenode"
    ports:
      - 4040:4040
      - 8888:8888
    deploy:
      resources:
        limits:
          memory: 512m #If limited, impossible to read the entire dataset to memory with pandas DF / need around 3g to be able to read the entire dataframe in pandas